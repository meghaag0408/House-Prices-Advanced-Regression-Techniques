{
  "name": "House-prices-advanced-regression-techniques",
  "tagline": "",
  "body": "# House-Prices : Advanced Regression Techniques\r\n\r\nSold!! How do home features add up to its price tag??#KAGGLE COMPETITION\r\n\r\n## Objective:\r\n\r\nWith 79 explanatory variables describing (almost) every aspect of residential homes in Ames, Iowa, this competition challenges you to predict the final price of each home.\r\nThe potential for creative feature engineering provides a rich opportunity for fun and learning using basic and combinations of regression techniques.\r\n\r\n\r\n## Data Exploration :\r\n\r\n* The train and the test data is concatenated.\r\n* NaN values are replaced with significant values.\r\n* The categorical variables which have some order(e.g. Poor, good, excellent) are replaced by numeric values [0, 1, 2, …..]\r\n* Yes/No in certain features are replaced by 0, 1.\r\n\r\n\r\n## New Feature Creation: \r\n\r\n\r\n* The feature extraction was quite minimal taking log transformations of numeric features and replacing missing values with the mean.\r\n* It Didn't worked well - To improve it we added New features to it.\r\n* Ordered categorical featured are divided into good and poor. The idea is good quality should rise price, poor quality - reduce price.\r\n* Exterior1st, Exterior2nd, RoofMatl, Condition1, Condition2, BldgType are converted to price brackets using SVM.\r\n* All the features are added to the original feature set, combining with different combinations of original features.\r\n* The final shape of the training data set was 1459 * 460 columns. \r\n\r\n\r\n## Scaling Features\r\n\r\n- First we have transformed the skewed numeric features(skew value is greater than 0.75) by taking log(feature + 1) - this will make the features more normal.\r\n- Logarithm is applied to target value as well.\r\n- Dummy variables for the categorical features are created.\r\n- The insignificant features are removed, which have many zeroes.\r\n- Missing values are replaced by mean whereever left.\r\n- The outliers id’s are identified and dropped. \r\n\r\n\r\n## Models\r\n\r\nWe have used 3 different types of models :-\r\n- #Linear regression with Lasso Regularization - The idea is to try Lasso a few times on bootstrapped samples and see how stable the feature selection is. As we have mostly tuned the parameters and Lasso should perform better. So giving more weight to it .\r\n- #Xgboost model to our linear model to see if we can improve the score giving weight to its prediction \r\n- #Elastic net Which is a regularized regression method that linearly combines the L1 and L2 penalties of the lasso and ridge methods.\r\n\r\n\r\n## Prediction\r\n\r\n* No single model gave efficient outputs.\r\n* Have tried with combinations of different models using basic ensemble methods.\r\n* Final prediction is done with giving different weights to the three models as:\r\n\tw1*lasso_pred + w2*elastic_net + w3*xgboost\t\twhere, w1+w2+w3 = 1\r\n\r\n\r\n#Libraries and Packages Required :\r\n \r\n* Skitlearn `>= 0.18`\r\n* Numpy  `>=1.11.2`\r\n* Pands `>=0.18.1`\r\n* scipy `>=0.18.1`\r\n* Xgboost `>=0.6`\r\n* seaborn `>=0.7.1`\r\n* More imports are :- itertools , operator\r\n",
  "note": "Don't delete this file! It's used internally to help with page regeneration."
}